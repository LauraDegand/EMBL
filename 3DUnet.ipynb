{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72d46dad-46a3-46d4-a9d0-f681e5773f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch_em\n",
    "from torch_em.model import UNet3d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d15c0b4b-a3f1-4fec-9115-7193c0997161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import os\n",
    "import imageio\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import binary_erosion\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1cf2fa3f-2ae6-4b7a-b541-969f7f4a21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images = glob(os.path.join('./rescaled' , 'train' , 'images' , '*.tif'))\n",
    "#train_labels = glob(os.path.join('./rescaled' , 'train' , 'labels' , '*.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec108fd5-0884-4d78-bb2d-cd90d1fe0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = './rescaled'\n",
    "train_images_path = os.path.join(dataset , 'train' , 'images')\n",
    "val_images_path = os.path.join(dataset , 'validation' , 'images')\n",
    "test_images_path = os.path.join(dataset , 'test' , 'images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d35a19f9-8536-4315-845d-3a5f441680f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrosophilaDataset(Dataset): #Dataset from torch\n",
    "    def __init__(self , root_dir , transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = os.listdir(root_dir)\n",
    "        self.transform = transform\n",
    "        #self.inp_transforms = transforms.Compose([transforms.Grayscale(), # some of the images are RGBtransforms.ToTensor(),transforms.Normalize([0.5], [0.5])])\n",
    "        #self.inp_transforms = transforms.ToTensor()\n",
    "        #self.mask_transforms = transforms.ToTensor()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self , idx):\n",
    "        im_name = self.samples[idx]\n",
    "        im_path = os.path.join(self.root_dir , im_name)\n",
    "        image = imageio.volread(im_path)\n",
    "        #image = np.expand_dims(image,axis=0)\n",
    "        image = torch.Tensor(image) # Transform image to tensor\n",
    "        \n",
    "        # Look at masks\n",
    "        labels_dir = self.root_dir.replace('images' , 'labels')\n",
    "        label_name = im_name.replace('Rec' , 'Rec_labels')\n",
    "        label_path = os.path.join(labels_dir , label_name)\n",
    "        \n",
    "        label = imageio.volread(label_path)\n",
    "        label = torch.Tensor(label)\n",
    "        #label = self.mask_transforms(label)\n",
    "        label[label == 4] = 3 #converts ovaries to same label\n",
    "        \n",
    "        image = torch.unsqueeze(image, dim=0)\n",
    "        #label = torch.unsqueeze(label, dim=0)\n",
    "\n",
    "    \n",
    "        return image , label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be587ba-77cf-4055-a11d-fa168d254f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_dataset_image(dataset):\n",
    "    idx = np.random.randint(0, len(dataset))    # take a random sample\n",
    "    img, label = dataset[idx]                    # get the image and the nuclei masks\n",
    "    f, axarr = plt.subplots(1, 2)               # make two plots on one figure\n",
    "    axarr[0].imshow(img[0])                     # show the image\n",
    "    axarr[1].imshow(label[0])                    # show the masks\n",
    "    _ = [ax.axis('off') for ax in axarr]        # remove the axes\n",
    "    print('Image size is %s' % {img[0].shape})\n",
    "    plt.show()\n",
    "    \n",
    "#show_random_dataset_image(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13ac70f9-4f75-4ed4-97c8-871b0faf1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DrosophilaDataset(root_dir = train_images_path)\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "val_data = DrosophilaDataset(root_dir = val_images_path)\n",
    "val_loader = DataLoader(val_data, batch_size=1)\n",
    "\n",
    "test_data = DrosophilaDataset(root_dir = test_images_path)\n",
    "test_loader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0c306-d6f2-475d-a822-abde5c1459f8",
   "metadata": {},
   "source": [
    "# U-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14ff8288-11fd-4009-ba59-e5d888af4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\" UNet implementation\n",
    "    Arguments:\n",
    "      in_channels: number of input channels\n",
    "      out_channels: number of output channels\n",
    "      final_activation: activation applied to the network output\n",
    "    \"\"\"\n",
    "   \n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                             nn.ReLU())       \n",
    "\n",
    "\n",
    "    # upsampling via transposed 3d convolutions\n",
    "    def _upsampler(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose3d(in_channels, out_channels,\n",
    "                                kernel_size=2, stride=2)\n",
    "    \n",
    "    def __init__(self, in_channels=1, out_channels=5, \n",
    "                 final_activation=None):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.depth = 4\n",
    "\n",
    "        # the final activation must either be None or a Module\n",
    "        if final_activation is not None:\n",
    "            assert isinstance(final_activation, nn.Module), \"Activation must be torch module\"\n",
    "        \n",
    "        # all lists of conv layers (or other nn.Modules with parameters) must be wraped\n",
    "        # itnto a nn.ModuleList\n",
    "        \n",
    "        # modules of the encoder path\n",
    "        self.encoder = nn.ModuleList([self._conv_block(in_channels, 16),\n",
    "                                      self._conv_block(16, 32),\n",
    "                                      self._conv_block(32, 64),\n",
    "                                      self._conv_block(64, 128)])\n",
    "        # the base convolution block\n",
    "        self.base = self._conv_block(128, 256)\n",
    "        # modules of the decoder path\n",
    "        self.decoder = nn.ModuleList([self._conv_block(256, 128),\n",
    "                                      self._conv_block(128, 64),\n",
    "                                      self._conv_block(64, 32),\n",
    "                                      self._conv_block(32, 16)])\n",
    "        \n",
    "        # the pooling layers; we use 2x2 MaxPooling\n",
    "        self.poolers = nn.ModuleList([nn.MaxPool3d(2) for _ in range(self.depth)])\n",
    "        # the upsampling layers\n",
    "        self.upsamplers = nn.ModuleList([self._upsampler(256, 128),\n",
    "                                         self._upsampler(128, 64),\n",
    "                                         self._upsampler(64, 32),\n",
    "                                         self._upsampler(32, 16)])\n",
    "        # output conv and activation\n",
    "        # the output conv is not followed by a non-linearity, because we apply\n",
    "        # activation afterwards\n",
    "        self.out_conv = nn.Conv3d(16, out_channels, 1)\n",
    "        self.activation = final_activation\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        # apply encoder path\n",
    "        encoder_out = []\n",
    "        for level in range(self.depth):\n",
    "            x = self.encoder[level](x)\n",
    "            encoder_out.append(x)\n",
    "            x = self.poolers[level](x)\n",
    "\n",
    "        # apply base\n",
    "        x = self.base(x)\n",
    "        \n",
    "        # apply decoder path\n",
    "        encoder_out = encoder_out[::-1]\n",
    "        for level in range(self.depth):\n",
    "            x = self.upsamplers[level](x)\n",
    "            x = self.decoder[level](torch.cat((x, encoder_out[level]), dim=1))\n",
    "        \n",
    "        # apply output conv and activation (if given)\n",
    "        x = self.out_conv(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f42abff-b801-4b96-a5ea-a847bd92d134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# check if we have  a gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20ac9a6f-1e7c-4666-96c3-b9786310050f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (base): Sequential(\n",
       "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (poolers): ModuleList(\n",
       "    (0): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (upsamplers): ModuleList(\n",
       "    (0): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (1): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (2): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (3): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "  )\n",
       "  (out_conv): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(in_channels = 1 , out_channels=4)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45b48c-caf7-4189-b771-3284cd693388",
   "metadata": {},
   "source": [
    "# Train & Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5d7c829c-cbc2-4c55-bcd4-4f7a35680c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer , epoch , tb_logger=tb_logger):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() #set the model to train mode\n",
    "    # iterate over the batches of this epoch\n",
    "    for batch_id, (x, y) in enumerate(dataloader):\n",
    "        # move input and target to the active device (either cpu or gpu)\n",
    "        #X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # zero the gradients for this iteration\n",
    "        #optimizer.zero_grad()\n",
    "        # apply model, calculate loss and run backwards pass\n",
    "        y = y.long()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        \n",
    "        prediction = model(x) # apply model\n",
    "        loss = loss_fn(prediction , y) # calculate loss\n",
    "        #loss = loss_fn(prediction , y[:0])#Expected target size [1, 128, 128, 128], got [1, 1, 128, 128, 128]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log to console\n",
    "        log_interval = 100\n",
    "        if batch_id % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                  epoch, batch_id * len(x),\n",
    "                  len(dataloader.dataset),\n",
    "                  100. * batch_id / len(dataloader), loss.item()))\n",
    "            \n",
    "        # Log to tensorboard\n",
    "        log_image_interval=20\n",
    "        if tb_logger is not None:\n",
    "            step = epoch * len(dataloader) + batch_id\n",
    "            tb_logger.add_scalar(tag='train_loss', scalar_value=loss.item(), global_step=step)\n",
    "            # check if we log images in this iteration\n",
    "            if step % log_image_interval == 0:\n",
    "                x_new = x[0][0][64]\n",
    "                x_new = x_new[None , None ,:]\n",
    "                tb_logger.add_images(tag='input', img_tensor = x_new.to('cpu'), global_step=step)\n",
    "                #tb_logger.add_images(tag='target', img_tensor = y.to('cpu'), global_step=step)\n",
    "                #tb_logger.add_images(tag='prediction', img_tensor=prediction.to('cpu').detach(), global_step=step)\n",
    "        \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "def validate(model, loader, loss_function, metric,  step=None, tb_logger=tb_logger):\n",
    "    model.eval\n",
    "    # running loss and metric values\n",
    "    val_loss = 0\n",
    "    val_metric = 0\n",
    "    \n",
    "    # disable gradients during validation\n",
    "    with torch.no_grad():\n",
    "        # iterate over validation loader and update loss and metric values\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            prediction = model(x)\n",
    "            y = y.long()\n",
    "            val_loss += loss_function(prediction , y).item()\n",
    "            val_metric += metric(prediction, y).item()\n",
    "\n",
    "    \n",
    "    # normalize loss and metric\n",
    "    val_loss /= len(loader)\n",
    "    val_metric /= len(loader)\n",
    "    \n",
    "    if tb_logger is not None:\n",
    "        tb_logger.add_scalar(tag='val_loss', scalar_value=val_loss, global_step=step)\n",
    "        tb_logger.add_scalar(tag='val_metric', scalar_value=val_metric, global_step=step)\n",
    "\n",
    "        \n",
    "    print('\\nValidate: Average loss: {:.4f}, Average Metric: {:.4f}\\n'.format(val_loss, val_metric))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c38f20f0-70b9-492e-bae3-512e74f6c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCoefficient(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    # the dice coefficient of two sets represented as vectors a, b ca be \n",
    "    # computed as (2 *|a b| / (a^2 + b^2))\n",
    "    def forward(self, prediction, target):\n",
    "        intersection = (prediction * target).sum()\n",
    "        denominator = (prediction * prediction).sum() + (target * target).sum()\n",
    "        return (2 * intersection / denominator.clamp(min=self.eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6aa52ec7-1db7-4943-9cfd-2f9527aec714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Epoch: 0 [0/56 (0%)]\tLoss: 0.301870\n",
      "\n",
      "Validate: Average loss: 0.3659, Average Metric: -0.0000\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Epoch: 1 [0/56 (0%)]\tLoss: 0.521286\n",
      "\n",
      "Validate: Average loss: 0.3471, Average Metric: -0.0000\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Epoch: 2 [0/56 (0%)]\tLoss: 0.170196\n",
      "\n",
      "Validate: Average loss: 0.3549, Average Metric: -0.0000\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Epoch: 3 [0/56 (0%)]\tLoss: 0.327845\n",
      "\n",
      "Validate: Average loss: 0.3415, Average Metric: -0.0000\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Epoch: 4 [0/56 (0%)]\tLoss: 0.185425\n",
      "\n",
      "Validate: Average loss: 0.3573, Average Metric: -0.0000\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Epoch: 5 [0/56 (0%)]\tLoss: 0.373617\n",
      "\n",
      "Validate: Average loss: 0.3270, Average Metric: -0.0000\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Epoch: 6 [0/56 (0%)]\tLoss: 0.373715\n",
      "\n",
      "Validate: Average loss: 0.3304, Average Metric: -0.0000\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Epoch: 7 [0/56 (0%)]\tLoss: 0.266415\n",
      "\n",
      "Validate: Average loss: 0.3871, Average Metric: -0.0000\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Epoch: 8 [0/56 (0%)]\tLoss: 0.299445\n",
      "\n",
      "Validate: Average loss: 0.3506, Average Metric: -0.0000\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Epoch: 9 [0/56 (0%)]\tLoss: 0.250928\n",
      "\n",
      "Validate: Average loss: 0.3438, Average Metric: -0.0000\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1.e-4) #and not e.-3\n",
    "metric = DiceCoefficient()\n",
    "\n",
    "n_epochs = 10 #Number of time the NN is iterate\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_function, optimizer , epoch , tb_logger=tb_logger)\n",
    "    #test_loop(test_loader, model, loss_function)\n",
    "    step = epoch * len(train_loader.dataset)\n",
    "    validate(model, test_loader, loss_function, metric, step, tb_logger=tb_logger)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f117018c-f10b-4cdb-8df2-9ad63fdb9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(UNet , \"model.pth\")\n",
    "# Load model\n",
    "#model = torch.load('model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f817e1b0-216b-4d9b-9434-206b6a1965c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a1a768d47734140\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a1a768d47734140\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6508;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tb_logger = SummaryWriter('runs/Unet')\n",
    "\n",
    "%tensorboard --logdir runs --port 6508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "89071cf6-bcdc-49ae-b9d6-e4b2103d2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kill 6880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ed838-a93b-4886-9c16-63d8307f042d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab9383-762d-413f-ad42-35f37c166c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a3fbe-4cc9-4b3c-aa72-66a97ccea913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL for Image Analysis (PyTorch)",
   "language": "python",
   "name": "dl-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
